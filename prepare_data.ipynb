{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import io\n",
    "import unidecode\n",
    "from typing import List, Dict\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "SUMMARY_PROMPT = \"\"\"\\\n",
    "Hier sind ein Text und einige Metadaten.\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "METADATEN:\n",
    "{metadata}\n",
    "\n",
    "AUFGABE:\n",
    "- Prüfe die Länge des obigen Textes.\n",
    "- Falls der Text weniger als 300 Tokens hat, gibt den ursprünglichen Text und die Metadaten unverändert zurück.\n",
    "- Falls der Text 300 Tokens oder mehr hat, schreibe eine prägnante und vollständige Zusammenfassung in 350-400 Tokens, \n",
    "  die alle relevanten Aspekte aus Text und Metadaten abdeckt. \n",
    "  Achte darauf, alle wichtigen Informationen klar wiederzugeben. \n",
    "  Die Zusammenfassung soll den ursprünglichen Inhalt möglichst gut repräsentieren.\n",
    "  Schreibe direkt die Zusammenfassung ohne davor mir mitzuteilen, wie viele Token, der Text hat. Starte mit \"Zusammenfassung:\"\n",
    "\n",
    "=================\n",
    "\"\"\"\n",
    "\n",
    "SUMMARY_TEMPLATE = ChatPromptTemplate.from_template(SUMMARY_PROMPT)\n",
    "\n",
    "LLM = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_key=\"Insert your Key\"\n",
    ")\n",
    "\n",
    "SUMMARIZER = SUMMARY_TEMPLATE | LLM\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Muster / Zeilen entfernen\n",
    "# ------------------------------------------------------------------------------\n",
    "PATTERNS_TO_REMOVE = [\n",
    "    r'^Druckdatum:\\s+[\\d\\.]+\\s+Seite:\\s+\\d+$',\n",
    "    r'^Prusseit\\s+u\\.\\s+R\\s*eiss\\s+Bauplanungsbüro\\s+GmbH$',\n",
    "    r'Gutenbergstr\\..*Garbsen.*Telefon.*Telefax',\n",
    "    r'e-mail:\\s*info@prusseitundreiss\\.de',\n",
    "    r'\\(Ort, Datum, Unterschrift und Stempel\\)',\n",
    "    r'^Leistungsverzeichnis\\s+Kurz-\\s*und\\s+Langtext$',\n",
    "    r'^Ordnungszahl\\s+Leistungsbeschreibung\\s+Menge\\s+ME\\s+Einheit\\s*spreis\\s+Gesamtbetrag$',\n",
    "    r'^in\\s+EUR\\s+in\\s+EUR$'\n",
    "]\n",
    "COMPILED_PATTERNS = [re.compile(pattern, re.IGNORECASE) for pattern in PATTERNS_TO_REMOVE]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Hilfsfunktionen\n",
    "# ------------------------------------------------------------------------------\n",
    "def save_json(data, filename: str):\n",
    "    \"\"\"Speichert die Daten als JSON-Datei (utf-8).\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"JSON gespeichert: {filename}\")\n",
    "\n",
    "\n",
    "def clean_metadata(metadata: Dict) -> Dict:\n",
    "    \"\"\"Säubert Metadaten (Unicode-Entfernung).\"\"\"\n",
    "    cleaned_metadata = {}\n",
    "    for key, value in metadata.items():\n",
    "        if isinstance(value, str):\n",
    "            normalized = unicodedata.normalize('NFKD', value)\n",
    "            cleaned_value = normalized.encode('ascii', 'ignore').decode('ascii')\n",
    "            cleaned_metadata[key] = cleaned_value\n",
    "        else:\n",
    "            cleaned_metadata[key] = str(value)\n",
    "    return cleaned_metadata\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Schritt 1: PDF einlesen + Textbereinigung\n",
    "# ------------------------------------------------------------------------------\n",
    "def read_and_clean_pdf(pdf_path: str) -> List[Dict]:\n",
    "    cleaned_pages = []\n",
    "\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        reader = PdfReader(pdf_file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for i in range(num_pages):\n",
    "            page_text = reader.pages[i].extract_text() or \"\"\n",
    "            lines = page_text.split('\\n')\n",
    "            cleaned_lines = []\n",
    "\n",
    "            for line in lines:\n",
    "                normalized_line = ' '.join(line.split())\n",
    "\n",
    "                # Entferne Zeilen, die den Mustern entsprechen\n",
    "                if any(pattern.search(normalized_line) for pattern in COMPILED_PATTERNS):\n",
    "                    continue\n",
    "\n",
    "                cleaned_lines.append(line)\n",
    "\n",
    "            cleaned_pages.append('\\n'.join(cleaned_lines))\n",
    "\n",
    "    full_cleaned_text = '\\n'.join(cleaned_pages)\n",
    "    metadata = {\n",
    "        'Dateiname': os.path.basename(pdf_path)\n",
    "    }\n",
    "\n",
    "    cleaned_metadata = clean_metadata(metadata)\n",
    "\n",
    "    return [{\n",
    "        'text': full_cleaned_text,\n",
    "        'metadata': cleaned_metadata\n",
    "    }]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Schritt 2: Inhaltsverzeichnis extrahieren\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_inhaltsverzeichnis(text: str):\n",
    "    pattern_start = re.compile(r'Inhaltsverzeichnis', re.IGNORECASE)\n",
    "    pattern_end = re.compile(r'Zusammenstellung.*(?:\\n|$)', re.IGNORECASE)\n",
    "\n",
    "    start_match = pattern_start.search(text)\n",
    "    if not start_match:\n",
    "        return None, text\n",
    "\n",
    "    start_index = start_match.start()\n",
    "    end_match = pattern_end.search(text, start_match.end())\n",
    "    end_index = end_match.end() if end_match else len(text)\n",
    "\n",
    "    ivz = text[start_index:end_index].strip()\n",
    "    rest = (text[:start_index] + text[end_index:]).strip()\n",
    "    return ivz, rest\n",
    "\n",
    "\n",
    "def process_inhaltsverzeichnis(docs: List[Dict]) -> List[Dict]:\n",
    "    new_data = []\n",
    "    for doc in docs:\n",
    "        text = doc.get('text', '')\n",
    "        metadata = doc.get('metadata', {})\n",
    "\n",
    "        ivz, rest_text = extract_inhaltsverzeichnis(text)\n",
    "        if ivz:\n",
    "            new_data.append({\n",
    "                'text': ivz,\n",
    "                'metadata': {**metadata, 'section': 'Inhaltsverzeichnis'}\n",
    "            })\n",
    "            doc['text'] = rest_text\n",
    "\n",
    "        new_data.append(doc)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Schritt 3: Zusätzliche Vorbemerkungen extrahieren\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_vorbemerkungen(text: str):\n",
    "    pattern_start = re.compile(r'Zusätzliche\\s+Vorbemerkungen', re.IGNORECASE)\n",
    "    pattern_end = re.compile(r'Baubeschreibung', re.IGNORECASE)\n",
    "\n",
    "    start_match = pattern_start.search(text)\n",
    "    if not start_match:\n",
    "        return [], text\n",
    "\n",
    "    start_index = start_match.end()\n",
    "    end_match = pattern_end.search(text, start_index)\n",
    "    end_index = end_match.start() if end_match else len(text)\n",
    "\n",
    "    segment_vorbem = text[start_index:end_index]\n",
    "    rest_text = text[:start_match.start()] + text[end_index:]\n",
    "\n",
    "    segment_vorbem = re.sub(r'\\s+', ' ', segment_vorbem)\n",
    "    list_item_pattern = re.compile(r'(\\d+)\\.\\s+(.*?)(?=\\s+\\d+\\.|\\Z)', re.DOTALL)\n",
    "    matches = list_item_pattern.findall(segment_vorbem)\n",
    "\n",
    "    vorbem_extracted = []\n",
    "    for number, txt in matches:\n",
    "        clean_item = re.sub(r'\\s+', ' ', txt).strip()\n",
    "        vorbem_extracted.append((number, clean_item))\n",
    "\n",
    "    return vorbem_extracted, rest_text.strip()\n",
    "\n",
    "\n",
    "def process_vorbemerkungen(docs: List[Dict]) -> List[Dict]:\n",
    "    new_data = []\n",
    "    for doc in docs:\n",
    "        text = doc.get('text', '')\n",
    "        metadata = doc.get('metadata', {})\n",
    "\n",
    "        vorbem, rest = extract_vorbemerkungen(text)\n",
    "        if vorbem:\n",
    "            for number, content in vorbem:\n",
    "                new_data.append({\n",
    "                    'text': content,\n",
    "                    'metadata': {**metadata, 'section': 'Zusätzliche Vorbemerkungen'}\n",
    "                })\n",
    "            doc['text'] = rest\n",
    "        new_data.append(doc)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Schritt 4: Baubeschreibung extrahieren\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_inhaltsverzeichnis_headings(docs: List[Dict]) -> List[str]:\n",
    "    headings = []\n",
    "    for doc in docs:\n",
    "        if doc['metadata'].get('section', '').lower() == 'inhaltsverzeichnis':\n",
    "            text = doc['text'].replace('\\n', ' ')\n",
    "            pattern = re.compile(r'(\\d+(?:\\.\\d+)*)\\.\\s+(.+?)\\s*\\.+\\s*\\d+')\n",
    "            found = pattern.findall(text)\n",
    "            for num, title in found:\n",
    "                headings.append(f\"{num}. {title.strip()}\")\n",
    "            break\n",
    "    return headings\n",
    "\n",
    "\n",
    "def extract_baubeschreibung(text: str, headings_level1: List[str]):\n",
    "    pattern_start = re.compile(r'Baubeschreibung', re.IGNORECASE)\n",
    "    start_match = pattern_start.search(text)\n",
    "    if not start_match:\n",
    "        return [], text\n",
    "\n",
    "    start_index = start_match.end()\n",
    "\n",
    "    numbering_patterns = []\n",
    "    pattern_number = re.compile(r'^(\\d+(?:\\.\\d+)*)\\.')\n",
    "    for heading in headings_level1:\n",
    "        match = pattern_number.match(heading)\n",
    "        if match:\n",
    "            number = re.escape(match.group(1) + '.')\n",
    "            numbering_patterns.append(number)\n",
    "\n",
    "    if not numbering_patterns:\n",
    "        end_index = len(text)\n",
    "    else:\n",
    "        combined = re.compile(r'(' + '|'.join(numbering_patterns) + r')\\s')\n",
    "        end_match = combined.search(text, start_index)\n",
    "        end_index = end_match.start() if end_match else len(text)\n",
    "\n",
    "    bb_text = text[start_index:end_index]\n",
    "    rest_text = text[:start_match.start()] + text[end_index:]\n",
    "\n",
    "    bb_text = re.sub(r'\\s+', ' ', bb_text)\n",
    "    subsection_pattern = re.compile(\n",
    "        r'(\\d+\\.\\d+)\\s+([^:]+):\\s+(.*?)(?=\\s+\\d+\\.\\d+\\s+[^:]+:|\\Z)',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    subsections = subsection_pattern.findall(bb_text)\n",
    "\n",
    "    cleaned_baubeschreibung = []\n",
    "    for number, header, inhalt in subsections:\n",
    "        clean_item = re.sub(r'\\s+', ' ', inhalt).strip()\n",
    "        cleaned_baubeschreibung.append((number, header.strip(), clean_item))\n",
    "\n",
    "    return cleaned_baubeschreibung, rest_text.strip()\n",
    "\n",
    "\n",
    "def process_baubeschreibung(docs: List[Dict]) -> List[Dict]:\n",
    "    headings = extract_inhaltsverzeichnis_headings(docs)\n",
    "    new_data = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.get('text', '')\n",
    "        metadata = doc.get('metadata', {})\n",
    "\n",
    "        bb, rest = extract_baubeschreibung(text, headings)\n",
    "        if bb:\n",
    "            for number, header, inhalt in bb:\n",
    "                new_data.append({\n",
    "                    'text': inhalt,\n",
    "                    'metadata': {**metadata, 'section': 'Baubeschreibung'}\n",
    "                })\n",
    "            doc['text'] = rest\n",
    "        new_data.append(doc)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Schritt 5: Ausschreibungstext extrahieren\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_inhaltsverzeichnis_headings_level1(docs: List[Dict]) -> List[str]:\n",
    "    headings = []\n",
    "    for doc in docs:\n",
    "        if doc['metadata'].get('section', '').lower() == 'inhaltsverzeichnis':\n",
    "            text = re.sub(r'\\s+', ' ', doc['text'])\n",
    "            pattern = re.compile(r'(\\d+)\\.\\s+(.+?)\\s*\\.+\\s*\\d+')\n",
    "            found = pattern.findall(text)\n",
    "            for num, title in found:\n",
    "                headings.append(f\"{num}. {title.strip()}\")\n",
    "            break\n",
    "    return headings\n",
    "\n",
    "\n",
    "def extract_ausschreibungstext(text: str, headings_level1: List[str]):\n",
    "    if not headings_level1:\n",
    "        return None, text\n",
    "\n",
    "    patterns = []\n",
    "    for heading in headings_level1:\n",
    "        pat = re.escape(heading).replace(r'\\ ', r'\\s+')\n",
    "        patterns.append(pat)\n",
    "    combined_pattern = re.compile(r'(' + '|'.join(patterns) + r')', re.IGNORECASE)\n",
    "\n",
    "    match = combined_pattern.search(text)\n",
    "    if not match:\n",
    "        return None, text\n",
    "\n",
    "    start_index = match.start()\n",
    "    ausschreibung = text[start_index:].strip()\n",
    "    rest = text[:start_index].strip()\n",
    "\n",
    "    return ausschreibung, rest\n",
    "\n",
    "\n",
    "def extract_subchapters(ausschreibungstext: str, base_metadata: Dict) -> List[Dict]:\n",
    "    chunks = []\n",
    "    if not ausschreibungstext.strip():\n",
    "        return [{'text': ausschreibungstext, 'metadata': base_metadata}]\n",
    "\n",
    "    heading_pattern = re.compile(\n",
    "        r'(?P<level>(\\d+\\.\\d+\\.\\d+\\.|\\d+\\.\\d+\\.))\\s*(?P<title>[^\\n]+)(?:\\r?\\n)+',\n",
    "        re.MULTILINE\n",
    "    )\n",
    "\n",
    "    matches = list(heading_pattern.finditer(ausschreibungstext))\n",
    "    if not matches:\n",
    "        return [{\n",
    "            'text': ausschreibungstext.strip(),\n",
    "            'metadata': {**base_metadata}\n",
    "        }]\n",
    "\n",
    "    last_index = 0\n",
    "    current_subsection = None\n",
    "    current_subsubsection = None\n",
    "\n",
    "    for m in matches:\n",
    "        level = m.group('level').strip()\n",
    "        title = m.group('title').strip()\n",
    "        start_index = m.start()\n",
    "\n",
    "        # Alles bis zum Start dieser Überschrift = separater Chunk\n",
    "        if last_index < start_index:\n",
    "            text_segment = ausschreibungstext[last_index:start_index].strip()\n",
    "            if text_segment:\n",
    "                chunks.append({\n",
    "                    'text': text_segment,\n",
    "                    'metadata': {\n",
    "                        **base_metadata,\n",
    "                        'subsection': current_subsection,\n",
    "                        'subsubsection': current_subsubsection\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        level_depth = level.count('.')\n",
    "        if level_depth == 2:\n",
    "            current_subsection = f\"{level} {title}\"\n",
    "            current_subsubsection = None\n",
    "        elif level_depth == 3:\n",
    "            current_subsubsection = f\"{level} {title}\"\n",
    "\n",
    "        last_index = start_index\n",
    "\n",
    "    # Den Rest anhängen\n",
    "    if last_index < len(ausschreibungstext):\n",
    "        text_segment = ausschreibungstext[last_index:].strip()\n",
    "        if text_segment:\n",
    "            chunks.append({\n",
    "                'text': text_segment,\n",
    "                'metadata': {\n",
    "                    **base_metadata,\n",
    "                    'subsection': current_subsection,\n",
    "                    'subsubsection': current_subsubsection\n",
    "                }\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_ausschreibungstext(docs: List[Dict]) -> List[Dict]:\n",
    "    headings_level1 = extract_inhaltsverzeichnis_headings_level1(docs)\n",
    "    new_data = []\n",
    "\n",
    "    for doc in docs:\n",
    "        md = doc.get('metadata', {})\n",
    "        if md.get('section') in ['Inhaltsverzeichnis', 'Zusätzliche Vorbemerkungen', 'Baubeschreibung']:\n",
    "            new_data.append(doc)\n",
    "            continue\n",
    "\n",
    "        text = doc.get('text', '')\n",
    "        ausschreibung, rest = extract_ausschreibungstext(text, headings_level1)\n",
    "        if ausschreibung:\n",
    "            subchapters = extract_subchapters(ausschreibung, {**md, 'section': 'Ausschreibungstext'})\n",
    "            new_data.extend(subchapters)\n",
    "\n",
    "            if rest:\n",
    "                doc['text'] = rest\n",
    "                new_data.append(doc)\n",
    "        else:\n",
    "            new_data.append(doc)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Schritt 6: Nummerierungen vereinheitlichen\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_numbering_and_remainder(full_str: str):\n",
    "    match = re.match(r'^\\s*([\\d\\.]+)\\s+(.*)', full_str)\n",
    "    if match:\n",
    "        numbering = match.group(1).strip()\n",
    "        remainder = match.group(2).strip()\n",
    "        return numbering, remainder\n",
    "    return None, full_str.strip()\n",
    "\n",
    "\n",
    "def unify_numberings_in_metadata(docs: List[Dict]) -> List[Dict]:\n",
    "    for doc in docs:\n",
    "        md = doc.get('metadata', {})\n",
    "        # subsection\n",
    "        if md.get('subsection'):\n",
    "            num, remainder = extract_numbering_and_remainder(md['subsection'])\n",
    "            if num:\n",
    "                md['subsection_number'] = num\n",
    "                md['subsection'] = remainder\n",
    "\n",
    "        # subsubsection\n",
    "        if md.get('subsubsection'):\n",
    "            num, remainder = extract_numbering_and_remainder(md['subsubsection'])\n",
    "            if num:\n",
    "                md['subsubsection'] = remainder\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  SCHRITT \"X\": Entfernen von PDF-Inhaltsverzeichnis-Fragmenten (Junk)\n",
    "# ------------------------------------------------------------------------------\n",
    "def remove_junk_chunks(docs: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Entfernt zwei Arten von Junk-Chunks:\n",
    "      1) Einzeilig, Muster: \"1.XX. <Titel> ...........\"\n",
    "         - Muss relativ kurz sein (z.B. < 200 Zeichen)\n",
    "         - Erkennung via single_line_pattern\n",
    "      2) Mehrzeiliges Zusammenstellungs-Fragment mit 'Zusammenstellung' und 'Projekt:'\n",
    "         - Muss relativ kurz sein (z.B. < 600 Zeichen)\n",
    "         - Muss mindestens eine Nummerierung (^\\s*\\d+\\.\\d+) enthalten.\n",
    "    \"\"\"\n",
    "    # RegEx für Fall (1): Nummerierung + Titel + viele Punkte + Zeilenende\n",
    "    single_line_pattern = re.compile(\n",
    "        r'^[0-9]+\\.[0-9]+(\\.[0-9]+)*\\.?\\s+.+\\.+\\s*$'\n",
    "    )\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    for doc in docs:\n",
    "        txt = doc.get(\"text\", \"\").strip()\n",
    "        if not txt:\n",
    "            cleaned.append(doc)\n",
    "            continue\n",
    "\n",
    "        lines = txt.splitlines()\n",
    "\n",
    "        # --- (1) Erkennung: einzelner, kurzer \"Zeile mit Nummerierung + Punktekette\" ---\n",
    "        # Nur wenn es exakt 1 Zeile gibt ODER die erste Zeile sich so präsentiert\n",
    "        # (je nachdem, wie streng Sie sein wollen).\n",
    "        # Hier: wir verlangen genau 1 Zeile und <200 Zeichen.\n",
    "        single_line_match = (\n",
    "            len(lines) == 1\n",
    "            and single_line_pattern.match(lines[0])\n",
    "            and len(txt) < 200\n",
    "        )\n",
    "\n",
    "        # --- (2) Erkennung: mehrzeiliges \"Zusammenstellung/Projekt\"-Fragment ---\n",
    "        # Heuristik: Chunk enthält \"Zusammenstellung\" UND \"Projekt:\" UND mind. eine Nummerierung\n",
    "        #           und ist <600 Zeichen lang.\n",
    "        has_zusammenstellung = \"zusammenstellung\" in txt.lower()\n",
    "        has_projekt = \"projekt:\" in txt.lower()\n",
    "        matches_numbering = bool(re.search(r'^\\s*\\d+\\.\\d+', txt, re.MULTILINE))\n",
    "        is_short = len(txt) < 600\n",
    "\n",
    "        multi_line_match = (\n",
    "            has_zusammenstellung\n",
    "            and has_projekt\n",
    "            and matches_numbering\n",
    "            and is_short\n",
    "        )\n",
    "\n",
    "        # => Falls einer der beiden Fälle zutrifft: Junk -> entfernen\n",
    "        if single_line_match or multi_line_match:\n",
    "            continue\n",
    "\n",
    "        # Sonst behalten wir den Chunk\n",
    "        cleaned.append(doc)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Schritt 7: ASCII-Konvertierung\n",
    "# ------------------------------------------------------------------------------\n",
    "def ensure_ascii_conformance(docs: List[Dict]) -> List[Dict]:\n",
    "    new_data = []\n",
    "    for doc in docs:\n",
    "        ascii_text = unidecode.unidecode(doc['text'])\n",
    "        ascii_md = {}\n",
    "        for k, v in doc.get('metadata', {}).items():\n",
    "            ascii_k = unidecode.unidecode(str(k))\n",
    "            ascii_v = unidecode.unidecode(str(v))\n",
    "            ascii_md[ascii_k] = ascii_v\n",
    "\n",
    "        new_data.append({'text': ascii_text, 'metadata': ascii_md})\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#  Schritt 8: Zusammenfassung erstellen (mit Token-Logik)\n",
    "# ------------------------------------------------------------------------------\n",
    "try:\n",
    "    import tiktoken\n",
    "\n",
    "    def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "        return len(enc.encode(text))\n",
    "except ImportError:\n",
    "    def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "        words = len(text.split())\n",
    "        return int(words / 1.3)\n",
    "\n",
    "\n",
    "def make_summaries(docs: List[Dict]) -> List[Dict]:\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        txt = doc.get(\"text\", \"\").strip()\n",
    "        md = doc.get(\"metadata\", {})\n",
    "\n",
    "        meta_str = []\n",
    "        if \"Dateiname\" in md:\n",
    "            meta_str.append(f\"Dateiname: {md['Dateiname']}\")\n",
    "        if \"section\" in md:\n",
    "            meta_str.append(f\"section: {md['section']}\")\n",
    "        if \"subsection\" in md:\n",
    "            meta_str.append(f\"subsection: {md['subsection']}\")\n",
    "        if \"subsubsection\" in md:\n",
    "            meta_str.append(f\"subsubsection: {md['subsubsection']}\")\n",
    "        if \"subsection_number\" in md:\n",
    "            meta_str.append(f\"subsection_number: {md['subsection_number']}\")\n",
    "\n",
    "        meta_as_text = \"\\n\".join(meta_str).strip()\n",
    "        token_count = count_tokens(txt)\n",
    "\n",
    "        if token_count < 300:\n",
    "            doc[\"summary\"] = f\"{txt}\\n\\n[METADATEN]\\n{meta_as_text}\"\n",
    "        else:\n",
    "            prompt_input = {\"text\": txt, \"metadata\": meta_as_text}\n",
    "            result = SUMMARIZER.invoke(prompt_input)\n",
    "            doc[\"summary\"] = result.content\n",
    "\n",
    "        new_docs.append(doc)\n",
    "    return new_docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON gespeichert: step1_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"GU-LV -PBS Hallenanbau mit Mezzanine.pdf\"\n",
    "\n",
    "# 1) PDF einlesen und Bereinigung\n",
    "docs = read_and_clean_pdf(pdf_path)\n",
    "save_json(docs, \"step1_cleaned.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON gespeichert: step2_inhaltsverzeichnis.json\n",
      "JSON gespeichert: step3_vorbemerkungen.json\n"
     ]
    }
   ],
   "source": [
    "# 2) Inhaltsverzeichnis\n",
    "docs = process_inhaltsverzeichnis(docs)\n",
    "save_json(docs, \"step2_inhaltsverzeichnis.json\")\n",
    "\n",
    "# 3) Zusätzliche Vorbemerkungen\n",
    "docs = process_vorbemerkungen(docs)\n",
    "save_json(docs, \"step3_vorbemerkungen.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON gespeichert: step4_baubeschreibung.json\n",
      "JSON gespeichert: step5_ausschreibungstext.json\n",
      "JSON gespeichert: step6_numberings.json\n",
      "JSON gespeichert: step6_remove_chunks.json\n",
      "JSON gespeichert: step7_ascii.json\n"
     ]
    }
   ],
   "source": [
    "# 4) Baubeschreibung\n",
    "docs = process_baubeschreibung(docs)\n",
    "save_json(docs, \"step4_baubeschreibung.json\")\n",
    "\n",
    "# 5) Ausschreibungstext\n",
    "docs = process_ausschreibungstext(docs)\n",
    "save_json(docs, \"step5_ausschreibungstext.json\")\n",
    "\n",
    "# 6) Nummerierungen anpassen\n",
    "docs = unify_numberings_in_metadata(docs)\n",
    "save_json(docs, \"step6_numberings.json\")\n",
    "\n",
    "docs = remove_junk_chunks(docs)\n",
    "save_json(docs, \"step6_remove_chunks.json\")\n",
    "\n",
    "# 7) ASCII-Konformität\n",
    "docs = ensure_ascii_conformance(docs)\n",
    "save_json(docs, \"step7_ascii.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON gespeichert: step8_summaries.json\n"
     ]
    }
   ],
   "source": [
    "# 8) Zusammenfassungen erstellen\n",
    "docs = make_summaries(docs)\n",
    "save_json(docs, \"step8_summaries.json\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-Projekte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
